{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Archived Mini Normals from Mafiascum.net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrapy Structure/Lingo:\n",
    "**Spiders** extract data **items**, which Scrapy send one by one to a configured **item pipeline** (if there is possible) to do post-processing on the items.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant packages..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import math\n",
    "import logging\n",
    "import json\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.item import Item, Field\n",
    "from scrapy.selector import Selector\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial variables..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "perpage = 25\n",
    "\n",
    "class PostItem(scrapy.Item):\n",
    "    pagelink = scrapy.Field()\n",
    "    forum = scrapy.Field()\n",
    "    thread = scrapy.Field()\n",
    "    number = scrapy.Field()\n",
    "    timestamp = scrapy.Field()\n",
    "    user = scrapy.Field()\n",
    "    content = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define what happens to scrape output..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# The following pipeline stores all scraped items (from all spiders) \n",
    "# into a single items.jl file, containing one item per line serialized \n",
    "# in JSON format:\n",
    "class JsonWriterPipeline(object):\n",
    "\n",
    "    # operations performed when spider starts\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('../data/posts.jsonl', 'w')\n",
    "\n",
    "    # when the spider finishes\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    # when the spider yields an item\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define spider..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class MafiaScumSpider(scrapy.Spider):\n",
    "    name = 'mafiascum'\n",
    "     \n",
    "    # settings\n",
    "    custom_settings = {'LOG_LEVEL': logging.WARNING,\n",
    "                      'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}}\n",
    "    \n",
    "    def start_requests(self):\n",
    "        \n",
    "        # define set of threads we're going to scrape from (ie all of them)\n",
    "        urls = [each[:each.find('\\n')] for each in open('../data/archive.txt').read().split('\\n\\n\\n')]\n",
    "        for url in tqdm(urls):\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    # get page counts and then do the REAL parse on every single page\n",
    "    def parse(self, response):\n",
    "        # find page count \n",
    "        try:\n",
    "            postcount = Selector(response).xpath(\n",
    "                '//div[@class=\"pagination\"]/text()').extract()\n",
    "            postcount = int(postcount[0][4:postcount[0].find(' ')])\n",
    "\n",
    "            # yield parse for every page of thread\n",
    "            for i in range(math.ceil(postcount/perpage)):\n",
    "                yield scrapy.Request(response.url+'&start='+str(i*perpage),\n",
    "                                    callback=self.parse_page)\n",
    "        except IndexError: # if can't, the thread probably doesn't exist\n",
    "            return\n",
    "        \n",
    "        \n",
    "    def parse_page(self, response):\n",
    "        # scan through posts on page and yield Post items for each\n",
    "        sel = Selector(response)\n",
    "        location = sel.xpath('//div[@id=\"page-body\"]/h2/a/@href').extract()[0]\n",
    "        forum = location[location.find('f=')+2:location.find('&t=')]\n",
    "        if location.count('&') == 1:\n",
    "            thread = location[location.find('&t=')+3:]\n",
    "        elif location.count('&') == 2:\n",
    "            thread = location[\n",
    "                location.find('&t=')+3:location.rfind('&')]\n",
    "        \n",
    "        posts = (sel.xpath('//div[@class=\"post bg1\"]') +\n",
    "                 sel.xpath('//div[@class=\"post bg2\"]'))\n",
    "        \n",
    "        for p in posts:\n",
    "            post = PostItem()\n",
    "            post['forum'] = forum\n",
    "            post['thread'] = thread\n",
    "            post['pagelink'] = response.url\n",
    "            try:\n",
    "                post['number'] = p.xpath(\n",
    "                    'div/div[@class=\"postbody\"]/p/a[2]/strong/text()').extract()[0][1:]\n",
    "            except IndexError:\n",
    "                post['number'] = p.xpath(\n",
    "                    'div[@class=\"postbody\"]/p/a[2]/strong/text()').extract()[0][1:]\n",
    "            \n",
    "            try:\n",
    "                post['timestamp'] = p.xpath(\n",
    "                    'div/div/p/text()[4]').extract()[0][23:-4]\n",
    "            except IndexError:\n",
    "                post['timestamp'] = p.xpath(\n",
    "                    'div[@class=\"postbody\"]/p/text()[4]').extract()[0][23:-4]\n",
    "            \n",
    "            try:\n",
    "                post['user'] = p.xpath('div/div/dl/dt/a/text()').extract()[0]\n",
    "            except IndexError:\n",
    "                post['user'] = '<<DELETED_USER>>'\n",
    "                \n",
    "            try:\n",
    "                post['content'] = p.xpath(\n",
    "                    'div/div/div[@class=\"content\"]').extract()[0][21:-6]\n",
    "            except IndexError:\n",
    "                post['content'] = p.xpath(\n",
    "                    'div[@class=\"postbody\"]/div[@class=\"content\"]').extract()[0][21:-6]\n",
    "            \n",
    "            yield post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start scraping..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-26 03:23:37 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: scrapybot)\n",
      "2020-01-26 03:23:37 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0\n",
      "2020-01-26 03:23:37 [scrapy.crawler] INFO: Overridden settings: {'LOG_LEVEL': 30, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2020-01-26 03:23:41 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://forum.mafiascum.net/viewtopic.php?f=53&t=16852&start=1300. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "\n",
    "process.crawl(MafiaScumSpider)\n",
    "process.start()\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and output should be a json file in same directory as this notebook! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Results into Unique Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'posts.jl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e37044743ce3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mposts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'posts.jl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpost\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mposts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'posts/{}.jsonl'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'thread'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'posts.jl'"
     ]
    }
   ],
   "source": [
    "posts = open('posts.jl')\n",
    "for post in posts:\n",
    "    with open('posts/{}.jsonl'.format(json.loads(post)['thread']), 'a') as f:\n",
    "        f.write(post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leftover Code...\n",
    "Ah, this code is supposed to match each archive entry with a URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open mini normal archive\n",
    "\n",
    "# ??? i don't remember what this does; probably helped me collect archive links some time ago\n",
    "runthis = False\n",
    "\n",
    "if runthis:\n",
    "    # relevant packages\n",
    "    from selenium import webdriver\n",
    "    from scrapy.selector import Selector\n",
    "    import re\n",
    "\n",
    "    # configure browser\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.binary_location = '/Applications/Google Chrome Canary.app/Contents/MacOS/Google Chrome Canary'\n",
    "    options.add_argument('window-size=800x841')\n",
    "    driver = webdriver.Chrome(chrome_options=options)\n",
    "\n",
    "    # get the thread titles and links\n",
    "    links = []\n",
    "    titles = []\n",
    "    for i in range(0, 400, 100):\n",
    "        driver.get('https://forum.mafiascum.net/viewforum.php?f=53&start=' + str(i))\n",
    "        sel = Selector(text=driver.page_source)\n",
    "        links += sel.xpath('//div[@class=\"forumbg\"]/div/ul[@class=\"topiclist topics\"]/li/dl/dt/a[1]/@href').extract()\n",
    "        titles += sel.xpath('//div[@class=\"forumbg\"]/div/ul[@class=\"topiclist topics\"]/li/dl/dt/a[1]/text()').extract()\n",
    "\n",
    "    # formatting, excluding needless threads...\n",
    "    titles = titles[1:]\n",
    "    links = links[1:]\n",
    "    del links[titles.index('Mini Normal Archives')]\n",
    "    del titles[titles.index('Mini Normal Archives')]\n",
    "    titles = [re.search(r'\\d+', each).group(0) for each in titles]\n",
    "\n",
    "    # match txt archive game numbers with forum archive game numbers to find links\n",
    "    f = open('archive.txt', 'r')\n",
    "    txtarchives = f.read().split('\\n\\n\\n')\n",
    "    numbers = [re.search(r'\\d+', each[:each.find('\\n')]).group(0) for each in txtarchives]\n",
    "    f.close()\n",
    "\n",
    "    # store the result...\n",
    "    for i, n in enumerate(numbers):\n",
    "        txtarchives[i] = 'http://forum.mafiascum.net' + links[titles.index(n)][1:] + '\\n' + txtarchives[i]\n",
    "    f = open('archive2.txt', 'w')\n",
    "    f.write('\\n\\n\\n'.join(txtarchives))\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
